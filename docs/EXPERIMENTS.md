# Эксперименты
Здесь описаны подробности проведенных экспериментов.
Все эксперименты производились на следущей рабочей станции:
- СPU: Ryzen 9700x
- RAM: 32GB DDR5
- GPU: NVIDIA RTX 5060TI
- OS: Linux

## Эксперимент 1

Разметка данных производилась в помощью zero-shot метода и одной модели -- **Grounding Dino**.
В модель для разметки данных передавался текстовый запрос (например "T-Bank logo . T-Bank . shield logo with T"),
на основе которого были получены bounding boxes.

**Результат**: разметка данных очень низкого качества, Grounding Dino не смог справиться с задачей нахождения конкретного лого.

## Эксперимент 2

Разметка данных производилась с помощью комбинированного подхода -- **Grounding Dino** использовался для выделения boxes со всеми логотипами,
далее использовалась модель **CLIP** фильтрации логотипов.

CLIP модель извлекала features из изображения и двух наборов текстовых запросов -- "За" и "Против".
После этого рассчитывалось косинусное сходство между features изображения и features текстовых наборов.
Если наибольшее сходство было с запросом из группы "За", изображение помечалось как содержащее логотип T-Банка.

В рамках данного эксперимента подбирались различные параметры пороговых значений текста и bounding box для модели Grounding Dino,
а также возможные варианты текстовых запросов. Итоговую реализацию можно посмотреть в исходном коде модуля data -- см. класс LegacyAnnotator.

Далее обучалась модель YOLOv8 с оптимизатором AdamW и размером batch = 16. Результат 50 эпох обучения: MAP ~ 0.67.

Недостатком данного подхода был плохой **Recall** при относительно неплохом **Precision** (наблюдения на основе ручного анализа разметки) -- среди отмеченных как имеющие логотип Т-Банк изображений было много лишних. Также как логотип помечались изображения, содержащие не только его (box был больше чем сам логотип).

## Эксперимент 3

Аналогично эксперименту 2, для разметки данных использовались Grounding Dino и CLIP модели.
Основным отличием является метод отбора подходящих изображений с помощью CLIP.
Теперь был создан reference набор изображений с логотипом Т-Банк, из которых CLIP извлекала референсные features.
На основе косинусного сходства features данного изображения с features референсных изображений происходила фильтрация логотипов.

В результате было достигнуто лучшее по сравнению с экспериментом 2 качество разметки данных.
Модель YOLOv11 показала лучшую обобщающую способность на улучшенной разметке (MAP ~ 0.94 на основе 30 эпох обучения).

## Эксперимент 4

К вышеописанному в эксперименте 3 было расширено использование аугментации.
Подробнее см. в файл [конфигурации модели](../configs/yolo_config.yaml).

# Метрики последнего цикла обучения модели

Основные валидационные метрики:

<img src="assets/metrics/results.jpg" width="800" />

Precision-Recall Curve:

<img src="assets/metrics/roc.jpg" width="800" />
